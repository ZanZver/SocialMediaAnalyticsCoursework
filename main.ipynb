{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',504)\n",
    "pd.set_option('display.width',1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Statistical analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are popular trends on Twitter at the moment, either in the UK or worldwide?\n",
    "Extract some insights from these trends such as: when it started in each place?\n",
    "What devices are used to tweet? and what sources can you trust? Use plots, graphs\n",
    "and maps to explain your insights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from creds import bot_username, bot_pass, bot_ID, bot_token\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import squarify    # pip install squarify (algorithm for treemap)\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib.dates import MonthLocator, DateFormatter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_reddit_api():\n",
    "    return praw.Reddit(\n",
    "    client_id=bot_ID,\n",
    "    client_secret=bot_token,\n",
    "    password=bot_pass,\n",
    "    user_agent=bot_ID,\n",
    "    username=bot_username,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_chars(text):\n",
    "    myDict = {  \n",
    "                \"\\u2019\"  : \"'\", \n",
    "                \"\\u200b\" : \"\",\n",
    "                \"\\u00a3\" : \"\",\n",
    "                \"\\u201d\" : \"\",\n",
    "                \"\\u201c\" : \"\",\n",
    "                \"\\u2018\" : \"\",\n",
    "                \"\\u2026\" : \"\",\n",
    "                \"\\n\" : \"\",\n",
    "                '''\"''' : \"'\",\n",
    "                \"\\\\\" : \"'\"\n",
    "            }\n",
    "    return \"\".join([myDict.get(c, c) for c in text])    \n",
    "\n",
    "def dict_filter(original_dict):\n",
    "    return_list = []\n",
    "    for i in original_dict:\n",
    "        temp_dict = {}\n",
    "        temp_dict[\"coin_price\"] = i[\"coin_price\"]\n",
    "        temp_dict[\"coin_reward\"] = i[\"coin_reward\"]\n",
    "        temp_dict[\"description\"] = i[\"description\"]\n",
    "        temp_dict[\"count\"] = i[\"count\"]\n",
    "        temp_dict[\"name\"] = i[\"name\"]\n",
    "        temp_dict[\"award_type\"] = i[\"award_type\"]\n",
    "        return_list.append(temp_dict)\n",
    "    return(return_list)\n",
    "    \n",
    "def get_reddit_data(filter_type = \"new\", subreddit_name = \"ethereum\"):\n",
    "    reddit = set_reddit_api()\n",
    "    li = []\n",
    "    int_checker = lambda x: x if x != None else 0\n",
    "    list_checker = lambda x: x if len(x) != 0 else []\n",
    "    str_checker = lambda x: x if x != None else \"u/[deleted]\"\n",
    "    submissions = None\n",
    "    if filter_type == \"new\":\n",
    "        submissions = reddit.subreddit(subreddit_name).new(limit=2000)\n",
    "    elif filter_type == \"top_all\":\n",
    "        submissions = reddit.subreddit(subreddit_name).top(time_filter=\"all\", limit = 2000)\n",
    "    \n",
    "    for submission in submissions:\n",
    "        temp_dict =  {\n",
    "            #\"all_awardings\": dict_filter(list(list_checker(submission.all_awardings))),\\\n",
    "            \"author\": str(str_checker(submission.author)),\\\n",
    "            \"author_is_blocked\": bool(submission.author_is_blocked),\\\n",
    "            \"awarders\": list(list_checker(submission.awarders)),\\\n",
    "            \"banned_at_utc\": datetime.strptime(time.strftime('%d %b %Y %H:%M:%S', time.localtime(submission.banned_at_utc)), '%d %b %Y %H:%M:%S'),\\\n",
    "            \"banned_by\": str(str_checker(submission.banned_by)),\\\n",
    "            \"can_gild\": bool(submission.can_gild),\\\n",
    "            \"category\" : str(str_checker(submission.category)),\\\n",
    "            \"created_utc\": datetime.strptime(time.strftime('%d %b %Y %H:%M:%S', time.localtime(submission.created_utc)), '%d %b %Y %H:%M:%S'),\\\n",
    "            \"downs\": int(int_checker(submission.downs)),\\\n",
    "            \"edited\": bool(submission.edited),\\\n",
    "            \"is_crosspostable\": bool(submission.is_crosspostable),\\\n",
    "            \"is_video\": bool(submission.is_video),\\\n",
    "            \"locked\": bool(submission.locked),\\\n",
    "            \"mod_note\": str(str_checker(submission.mod_note)),\\\n",
    "            \"mod_reason_by\": str(str_checker(submission.mod_reason_by)),\\\n",
    "            \"mod_reason_title\": str(str_checker(submission.mod_reason_title)),\\\n",
    "            \"mod_reports\": str(str_checker(submission.mod_reports)),\\\n",
    "            \"num_comments\": int(int_checker(submission.num_comments)),\\\n",
    "            \"num_crossposts\": int(int_checker(submission.num_crossposts)),\\\n",
    "            \"num_duplicates\": int(int_checker(submission.num_duplicates)),\\\n",
    "            \"num_reports\": int(int_checker(submission.num_reports)),\\\n",
    "            \"over_18\": bool(submission.over_18),\\\n",
    "            \"removal_reason\": str(str_checker(submission.removal_reason)),\\\n",
    "            \"removed_by\": str(str_checker(submission.removed_by)),\\\n",
    "            \"removed_by_category\": str(str_checker(submission.removed_by_category)),\\\n",
    "            \"score\": int(int_checker(submission.score)),\\\n",
    "            \"selftext\": replace_chars(str(str_checker(submission.selftext))),\\\n",
    "            \"spoiler\": bool(submission.spoiler),\\\n",
    "            \"title\": replace_chars(str(str_checker(submission.title))),\\\n",
    "            \"top_awarded_type\" : str(str_checker(submission.top_awarded_type)),\\\n",
    "            \"total_awards_received\": int(int_checker(submission.total_awards_received)),\\\n",
    "            \"ups\": int(int_checker(submission.ups)),\\\n",
    "            \"upvote_ratio\": float(int_checker(submission.upvote_ratio)),\\\n",
    "            \"url\": str(str_checker(submission.url)),\\\n",
    "            \"user_reports\": list(list_checker(submission.user_reports)),\n",
    "        }\n",
    "        li.append(temp_dict)\n",
    "    return li\n",
    "\n",
    "def removeStopWords(txt):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_sentence_selftext = []\n",
    "    filtered_sentence_title = []\n",
    "    \n",
    "    for item in txt:\n",
    "        selftext_list = item[0].split()\n",
    "        selftext_title = item[1].split()\n",
    "        for word in selftext_list:        \n",
    "            if word not in stop_words:\n",
    "                filtered_sentence_selftext.append(word)\n",
    "        for word in selftext_title:        \n",
    "            if word not in stop_words:\n",
    "                filtered_sentence_title.append(word)\n",
    "    \n",
    "    return filtered_sentence_selftext, filtered_sentence_title\n",
    "\n",
    "def get_top_mentioned_items(lst, le = 5):\n",
    "    return Counter(lst).most_common()[0:le]\n",
    "\n",
    "def save_data_locally(df,path):\n",
    "    df.to_csv(path, index=False)\n",
    "    \n",
    "def generate_tree_map(data_frame, item_1, item_2, item_1_txt, item_2_txt, title):\n",
    "    def format_label(item_1_val, item_2_val):\n",
    "        return f\"{item_1_txt}:\\n{item_1_val}\\n{item_2_txt}:\\n{item_2_val}\"\n",
    "\n",
    "    squarify.plot(sizes=data_frame[item_1], \n",
    "                label=[format_label(item_1, item_2) for item_1, item_2 \n",
    "                        in zip(data_frame[item_1], data_frame[item_2])],\n",
    "                color=sns.color_palette(\"pastel\", len(data_frame)))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f\"Graphs/A1/{item_1}_treemap.png\", dpi=600, bbox_inches='tight')\n",
    "    #plt.clear()\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "def generate_plot(data_frame, col_1, col_2, x_label, y_label, title):\n",
    "    plt.rcParams.update({'font.size': 12, 'font.family': 'serif'})\n",
    "    # Create bars\n",
    "    plt.bar(np.arange(len(data_frame[col_1])), data_frame[col_1],)\n",
    "\n",
    "    # Create names on the x-axis\n",
    "    plt.xticks(np.arange(len(data_frame[col_1])), data_frame[col_2])\n",
    "\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "\n",
    "    # Show graphic\n",
    "    #plt.show()\n",
    "    plt.savefig(f\"Graphs/A1/{col_1}_plot.png\", dpi=300, bbox_inches='tight', facecolor='white', transparent=False)\n",
    "    #plt.clear()\n",
    "    plt.close()\n",
    "    \n",
    "def generate_scatter_plot(data_frame, x_col, y_col, d_size, x_label, y_label, title):\n",
    "    # use the scatterplot function to build the bubble map\n",
    "    sns.scatterplot(data=data_frame, x=x_col, y=y_col, \n",
    "                    size=d_size, legend=False, sizes=(20, 2000), \n",
    "                    hue=d_size, \n",
    "                    palette = sns.color_palette(\"pastel\", len(data_frame)))\n",
    "    for i in range(len(data_frame)):\n",
    "        plt.annotate(data_frame[d_size][i], (data_frame[x_col][i], data_frame[y_col][i]))\n",
    "        \n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    # show the graph\n",
    "    #plt.show()\n",
    "    plt.savefig(f\"Graphs/A1/{x_col}_scatterplot.png\", dpi=300, bbox_inches='tight', facecolor='white', transparent=False)\n",
    "    #plt.clear()\n",
    "    plt.close()\n",
    "    \n",
    "def count_author_upvotes(data_frame):\n",
    "    # group by author and sum the upvotes\n",
    "    author_upvotes = data_frame.groupby('author')['ups'].sum().reset_index()\n",
    "    # count the number of occurrences of each author\n",
    "    author_counts = data_frame['author'].value_counts().reset_index()\n",
    "    author_counts.columns = ['author', 'number_of_occurrences']\n",
    "    # merge the two dataframes on author\n",
    "    result = pd.merge(author_upvotes, author_counts, on='author')\n",
    "    return result.sort_values(by=['ups'], ascending=False)\n",
    "\n",
    "def generate_scatter_plot_simple(data_frame,col_1, col_2, title):\n",
    "    # create a scatter plot with 'ups' on the x-axis and 'number_of_occurrences' on the y-axis\n",
    "    plt.scatter(data_frame[col_1], data_frame[col_2])\n",
    "\n",
    "    # set the axis labels and title\n",
    "    plt.xlabel(col_1)\n",
    "    plt.ylabel(col_2)\n",
    "    plt.title(title)\n",
    "\n",
    "    # display the plot\n",
    "    #plt.show()\n",
    "    plt.savefig(f\"Graphs/A1/{col_1}_scatterplot_simple.png\", dpi=300, bbox_inches='tight', facecolor='white', transparent=False)\n",
    "    #plt.clear()\n",
    "    plt.close()\n",
    "    \n",
    "def generate_double_histogram(data_frame, col_1, col_2, col_3, y_label_1, x_label, y_label_2, title):\n",
    "    # Create a figure with two subplots\n",
    "    # Create a figure with two subplots\n",
    "    data = data_frame\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    # Plot the first bar chart for num_crossposts\n",
    "    ax1.bar(range(len(data)), data[col_1])\n",
    "    ax1.set_xticks(range(len(data)))\n",
    "    ax1.set_xticklabels(data.index)\n",
    "    ax1.set_xlabel(x_label)\n",
    "    ax1.set_ylabel(y_label_1)\n",
    "\n",
    "    # Rotate the x-axis labels and add some spacing\n",
    "    ax1.tick_params(axis='x', rotation=90, pad=10)\n",
    "\n",
    "    # Plot the second bar chart for num_duplicates\n",
    "    ax2.bar(range(len(data)), data[col_3])\n",
    "    ax2.set_xticks(range(len(data)))\n",
    "    ax2.set_xticklabels(data.index)\n",
    "    ax2.set_xlabel(x_label)\n",
    "    ax2.set_ylabel(y_label_2)\n",
    "\n",
    "    # Rotate the x-axis labels and add some spacing\n",
    "    ax2.tick_params(axis='x', rotation=90, pad=10)\n",
    "    \n",
    "    # Adjust the layout and display the chart\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    #plt.show()\n",
    "    plt.savefig(f\"Graphs/A1/{col_1}_double_histogram.png\", dpi=300, bbox_inches='tight', facecolor='white', transparent=False)\n",
    "    #plt.clear()\n",
    "    plt.close()\n",
    "    \n",
    "def generate_timeseries_plot(data_frame, col_1, x_label, y_label, title):\n",
    "    # Convert the 'created_utc' column to datetime\n",
    "    data_frame['created_utc'] = pd.to_datetime(data_frame['created_utc'])\n",
    "\n",
    "    # Group the data by month and count the number of posts per month\n",
    "    counts = data_frame.groupby(pd.Grouper(key=col_1, freq='M')).size()\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Plot the data as a line chart\n",
    "    ax.plot(counts.index, counts.values)\n",
    "\n",
    "    # Set the x-axis ticks to show every 4 months\n",
    "    ax.xaxis.set_major_locator(MonthLocator(interval=4))\n",
    "\n",
    "    # Set the x-axis tick labels to display the month and year\n",
    "    ax.xaxis.set_major_formatter(DateFormatter('%b %Y'))\n",
    "\n",
    "    # Set the axis labels\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "\n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Show the plot\n",
    "    #plt.show()\n",
    "    plt.savefig(f\"Graphs/A1/{col_1}_timeseries.png\", dpi=300, bbox_inches='tight', facecolor='white', transparent=False)\n",
    "    #plt.clear()\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l2 = get_reddit_data()\n",
    "\n",
    "#top_eth = pd.DataFrame(get_reddit_data(filter_type=\"top_all\", subreddit_name=\"ethereum\"))\n",
    "reddit_data_frame = pd.read_csv(\"Data/Reddit/merged_subredits.csv\")\n",
    "#new_eth = pd.DataFrame(get_reddit_data(filter_type=\"new\", subreddit_name=\"ethereum\"))\n",
    "\n",
    "\n",
    "# Heatmap awards vs ups\n",
    "generate_tree_map(reddit_data_frame[[\"total_awards_received\",\"ups\"]].head(10),\n",
    "                  item_1 = \"ups\",\n",
    "                  item_2 = \"total_awards_received\",\n",
    "                  item_1_txt= \"Upvotes\",\n",
    "                  item_2_txt= \"Awards\", \n",
    "                  title = \"Upvotes and their rewards represented\")\n",
    "\n",
    "# Num of cross posts vs avards\n",
    "generate_plot(reddit_data_frame[[\"num_crossposts\",\"total_awards_received\"]].head(10),\n",
    "              col_1= \"num_crossposts\",\n",
    "              col_2= \"total_awards_received\",\n",
    "              x_label= \"Total awards received\",\n",
    "              y_label= \"Number of crossposts\",\n",
    "              title = \"Number of rewards received per top posts associated with number of crossposts\")\n",
    "\n",
    "# Num of comments vs cross posts vs ups\n",
    "generate_scatter_plot(reddit_data_frame[[\"num_crossposts\",\"num_comments\", \"ups\"]].head(10),\n",
    "                      x_col=\"num_crossposts\",\n",
    "                      y_col=\"num_comments\",\n",
    "                      d_size=\"ups\",\n",
    "                      x_label = \"Number of crossposts\", \n",
    "                      y_label = \"Number of comments\",\n",
    "                      title= \"Number of comments linked to number of crossposts and number of upvotes\")\n",
    "\n",
    "# Author post number vs number of upvotes\n",
    "#la3 = count_author_upvotes(top_eth).head(30)\n",
    "generate_scatter_plot_simple(count_author_upvotes(reddit_data_frame).head(30), \n",
    "                             col_1=\"ups\", \n",
    "                             col_2=\"number_of_occurrences\", \n",
    "                             title=\"Correlation between Ups and Number of occurrences for Authors\")\n",
    "\n",
    "# Num of duplicates vs num of crossposts\n",
    "lite_data = reddit_data_frame.sort_values(['num_crossposts', 'num_duplicates'], ascending=[False, False])\n",
    "generate_double_histogram(lite_data.head(15), \n",
    "                          col_1=\"num_crossposts\", \n",
    "                          col_2=\"Post\", \n",
    "                          col_3=\"num_duplicates\", \n",
    "                          y_label_1=\"Number of crossposts\", \n",
    "                          x_label= \"Posts\", \n",
    "                          y_label_2= \"Number of duplicates\",\n",
    "                          title=\"Posts crossposts compared to number of duplicates\")\n",
    "\n",
    "# Posts over time                          \n",
    "generate_timeseries_plot(reddit_data_frame, \n",
    "                         col_1=\"created_utc\", \n",
    "                         x_label = \"Date\", \n",
    "                         y_label = \"Number of Posts\", \n",
    "                         title = \"Posts Over Time\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use one of the graph datasets available in Stanford Large Network Dataset\n",
    "Collection (https://snap.stanford.edu/data/) or any other publicly available graph\n",
    "dataset (you can also create your own graph), apply the following:\n",
    "\n",
    "• Find the most important nodes (individuals) in the network based on different centrality measures, </br>\n",
    "• Visualise your graph using one of centrality measures of your choice, and </br>\n",
    "• Apply a Community Detection Algorithm to the graph, visualise the communities and discuss your findings.\n",
    "\n",
    "Use plots, graphs and maps to explain your insights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from creds import API_Key, urlBase\n",
    "import json\n",
    "import requests\n",
    "from decimal import Decimal\n",
    "from web3 import Web3\n",
    "import pandas as pd\n",
    "from pathlib import Path  \n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etherscan API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eth_block_number():\n",
    "    return str(urlBase + \n",
    "               \"?module=proxy\" + \n",
    "               \"&action=eth_blockNumber\" + \n",
    "               f\"&apikey={API_Key}\")\n",
    "\n",
    "def get_eth_block_by_number(block_number = \"0xfe22dd\"):\n",
    "    return str(urlBase + \n",
    "    \"?module=proxy\" +\n",
    "    \"&action=eth_getBlockByNumber\" +\n",
    "    f\"&tag={block_number}\" +\n",
    "    \"&boolean=true\" +\n",
    "    f\"&apikey={API_Key}\")\n",
    "    \n",
    "def get_ether_last_price():\n",
    "    return str(urlBase +\n",
    "    \"?module=stats\" +\n",
    "    \"&action=ethprice\" +\n",
    "    f\"&apikey={API_Key}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block():\n",
    "    # Format a json\n",
    "    format_as_json = json.dumps(requests.get(get_eth_block_number()).json())\n",
    "    # Load data as a json\n",
    "    load_as_json = json.loads(format_as_json)\n",
    "    #print(load_as_json[\"result\"])\n",
    "    return load_as_json\n",
    "\n",
    "def get_block_by_number(block_number = \"0xfe2385\"):\n",
    "    format_as_json = json.dumps(requests.get(get_eth_block_by_number(block_number)).json())\n",
    "    load_as_json = json.loads(format_as_json)\n",
    "    return load_as_json\n",
    "\n",
    "def get_eth_price():\n",
    "    # Format a json\n",
    "    format_as_json = json.dumps(requests.get(get_ether_last_price()).json())\n",
    "    # Load data as a json\n",
    "    load_as_json = json.loads(format_as_json)\n",
    "    #print(load_as_json[\"result\"])\n",
    "    return load_as_json\n",
    "\n",
    "def covert(amount, eth_price):\n",
    "    amount = int(amount, 0)\n",
    "    eth_amount = format(Web3.fromWei(amount, \"ether\"), '.18f')\n",
    "    usd_amount = float(eth_amount) * eth_price\n",
    "    return usd_amount\n",
    "\n",
    "def transfer_data_from(block_number):\n",
    "    df  = pd.DataFrame()\n",
    "    eth_price = float(get_eth_price()[\"result\"][\"ethusd\"])\n",
    "    try:\n",
    "        trx = get_block_by_number(block_number)[\"result\"][\"transactions\"]\n",
    "        for i in trx:\n",
    "            temp_df = pd.DataFrame([[i[\"from\"], \n",
    "                                     i[\"to\"], \n",
    "                                     covert(i[\"value\"], eth_price), \n",
    "                                     covert(i[\"gasPrice\"], eth_price)]],\n",
    "                                    columns=\n",
    "                                    [\"From\", \"To\", \"Value\", \"Gas_price\"])\n",
    "            df = pd.concat([df, temp_df])\n",
    "        return df\n",
    "    except:\n",
    "        return df\n",
    "\n",
    "def generate_data(block_number):\n",
    "    print(f\"Generating data for block: {block_number}\")\n",
    "    filepath = Path(f\"Data/Crypto/MultiBlocks/{block_number}.csv\")\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "    data = transfer_data_from(block_number)\n",
    "    data.to_csv(filepath)\n",
    "    \n",
    "def generate_blocks_in_range(block_range):\n",
    "    for i in block_range:\n",
    "        generate_data(i)\n",
    "\n",
    "def generate_block_list(block_str = \"0x1003e\", start_block = 10, end_block = 91):\n",
    "    block_range = []\n",
    "    [block_range.append(f\"{block_str}{i}\") for i in range (start_block, end_block)]\n",
    "    return block_range\n",
    "\n",
    "def generate_data_frame():\n",
    "    cols = [\"Unnamed\", \"From\", \"To\", \"Value\", \"Gas_price\"]\n",
    "    block_df = pd.DataFrame(columns=cols)\n",
    "    filenames= os.listdir(\"Data/Crypto/MultiBlocks\")\n",
    "    num_items = len(filenames)\n",
    "    start = 0\n",
    "    for filename in filenames: # loop through all the files and folders\n",
    "        start += 1\n",
    "        print(f\"{start}/{num_items}\")\n",
    "        if filename.endswith('.csv'):\n",
    "            block_df = block_df.append(pd.read_csv(f\"Data/Crypto/MultiBlocks/{filename}\"))\n",
    "\n",
    "    block_df = block_df.reset_index()\n",
    "    block_df = block_df.drop(['index', 'Unnamed', \"Unnamed: 0\"], axis=1)\n",
    "    return block_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_block()\n",
    "\n",
    "#generate_data_frame()\n",
    "big_df = generate_data_frame().rename(columns={\"From\": \"Source\", \"To\": \"Target\", \"Value\": \"Weight\"}).drop(columns=[\"Gas_price\"])\n",
    "big_df.insert(2, \"Type\", \"directed\")\n",
    "big_df.to_csv(\"Data/Crypto/BigBlock/big_df.csv\", index=False)\n",
    "\n",
    "#list_of_blocks = generate_block_list(block_str = \"0x10042\", start_block = 00, end_block = 99)\n",
    "#generate_blocks_in_range(list_of_blocks)\n",
    "#list_of_blocks = generate_block_list(block_str = \"0x10043\", start_block = 00, end_block = 99)\n",
    "#generate_blocks_in_range(list_of_blocks)\n",
    "#list_of_blocks = generate_block_list(block_str = \"0x10044\", start_block = 00, end_block = 99)\n",
    "#generate_blocks_in_range(list_of_blocks)\n",
    "#list_of_blocks = generate_block_list(block_str = \"0x10045\", start_block = 00, end_block = 99)\n",
    "#generate_blocks_in_range(list_of_blocks)\n",
    "#list_of_blocks = generate_block_list(block_str = \"0x10046\", start_block = 00, end_block = 99)\n",
    "#generate_blocks_in_range(list_of_blocks)\n",
    "#list_of_blocks = generate_block_list(block_str = \"0x10047\", start_block = 00, end_block = 99)\n",
    "#generate_blocks_in_range(list_of_blocks)\n",
    "#list_of_blocks = generate_block_list(block_str = \"0x10048\", start_block = 00, end_block = 99)\n",
    "#generate_blocks_in_range(list_of_blocks)\n",
    "#list_of_blocks = generate_block_list(block_str = \"0x10049\", start_block = 00, end_block = 99)\n",
    "#generate_blocks_in_range(list_of_blocks)\n",
    "#list_of_blocks = generate_block_list(block_str = \"0x1004a\", start_block = 00, end_block = 99)\n",
    "#generate_blocks_in_range(list_of_blocks)\n",
    "#list_of_blocks = generate_block_list(block_str = \"0x1004b\", start_block = 00, end_block = 99)\n",
    "#generate_blocks_in_range(list_of_blocks)\n",
    "#list_of_blocks = generate_block_list(block_str = \"0x1004c\", start_block = 00, end_block = 99)\n",
    "#generate_blocks_in_range(list_of_blocks)\n",
    "#list_of_blocks = generate_block_list(block_str = \"0x1004d\", start_block = 00, end_block = 99)\n",
    "#generate_blocks_in_range(list_of_blocks)\n",
    "#list_of_blocks = generate_block_list(block_str = \"0x1004e\", start_block = 00, end_block = 99)\n",
    "#generate_blocks_in_range(list_of_blocks)\n",
    "#list_of_blocks = generate_block_list(block_str = \"0x1004f\", start_block = 00, end_block = 99)\n",
    "#generate_blocks_in_range(list_of_blocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b = get_block_by_number(block_number=\"0x1024f51\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small_data = generate_data_frame().head(1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Text mining"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an event/campaign that happened in the UK or worldwide recently (i.e.,\n",
    "Brexit). Apply sentiment analysis to show users’ opinions about the topic on Twitter.\n",
    "Represent your findings using statistical descriptive methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zanzver/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/zanzver/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/zanzver/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/zanzver/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "from creds import bot_username, bot_pass, bot_ID, bot_token\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from datetime import datetime\n",
    "import time\n",
    "  # download the stopwords corpus\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')  # download the punkt tokenizer\n",
    "nltk.download('averaged_perceptron_tagger')  # download the POS tagger\n",
    "nltk.download('wordnet')  # download the WordNet lemmatizer and stemmer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_reddit_api():\n",
    "    return praw.Reddit(\n",
    "    client_id=bot_ID,\n",
    "    client_secret=bot_token,\n",
    "    password=bot_pass,\n",
    "    user_agent=bot_ID,\n",
    "    username=bot_username,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_chars(text):\n",
    "    myDict = {  \n",
    "                \"\\u2019\"  : \"'\", \n",
    "                \"\\u200b\" : \"\",\n",
    "                \"\\u00a3\" : \"\",\n",
    "                \"\\u201d\" : \"\",\n",
    "                \"\\u201c\" : \"\",\n",
    "                \"\\u2018\" : \"\",\n",
    "                \"\\u2026\" : \"\",\n",
    "                \"\\n\" : \"\",\n",
    "                '''\"''' : \"'\",\n",
    "                \"\\\\\" : \"'\"\n",
    "            }\n",
    "    return \"\".join([myDict.get(c, c) for c in text])    \n",
    "\n",
    "def dict_filter(original_dict):\n",
    "    return_list = []\n",
    "    for i in original_dict:\n",
    "        temp_dict = {}\n",
    "        temp_dict[\"coin_price\"] = i[\"coin_price\"]\n",
    "        temp_dict[\"coin_reward\"] = i[\"coin_reward\"]\n",
    "        temp_dict[\"description\"] = i[\"description\"]\n",
    "        temp_dict[\"count\"] = i[\"count\"]\n",
    "        temp_dict[\"name\"] = i[\"name\"]\n",
    "        temp_dict[\"award_type\"] = i[\"award_type\"]\n",
    "        return_list.append(temp_dict)\n",
    "    return(return_list)\n",
    "      \n",
    "def get_reddit_data(filter_type = \"new\", subreddit_name = \"ethereum\"):\n",
    "    reddit = set_reddit_api()\n",
    "    li = []\n",
    "    int_checker = lambda x: x if x != None else 0\n",
    "    list_checker = lambda x: x if len(x) != 0 else []\n",
    "    str_checker = lambda x: x if x != None else \"u/[deleted]\"\n",
    "    submissions = None\n",
    "    if filter_type == \"new\":\n",
    "        submissions = reddit.subreddit(subreddit_name).new(limit=2000)\n",
    "    elif filter_type == \"top_all\":\n",
    "        submissions = reddit.subreddit(subreddit_name).top(time_filter=\"all\", limit = 2000)\n",
    "    \n",
    "    for submission in submissions:\n",
    "        temp_dict =  {\n",
    "            #\"all_awardings\": dict_filter(list(list_checker(submission.all_awardings))),\\\n",
    "            \"author\": str(str_checker(submission.author)),\\\n",
    "            \"author_is_blocked\": bool(submission.author_is_blocked),\\\n",
    "            \"awarders\": list(list_checker(submission.awarders)),\\\n",
    "            \"banned_at_utc\": datetime.strptime(time.strftime('%d %b %Y %H:%M:%S', time.localtime(submission.banned_at_utc)), '%d %b %Y %H:%M:%S'),\\\n",
    "            \"banned_by\": str(str_checker(submission.banned_by)),\\\n",
    "            \"can_gild\": bool(submission.can_gild),\\\n",
    "            \"category\" : str(str_checker(submission.category)),\\\n",
    "            \"created_utc\": datetime.strptime(time.strftime('%d %b %Y %H:%M:%S', time.localtime(submission.created_utc)), '%d %b %Y %H:%M:%S'),\\\n",
    "            \"downs\": int(int_checker(submission.downs)),\\\n",
    "            \"edited\": bool(submission.edited),\\\n",
    "            \"is_crosspostable\": bool(submission.is_crosspostable),\\\n",
    "            \"is_video\": bool(submission.is_video),\\\n",
    "            \"locked\": bool(submission.locked),\\\n",
    "            \"mod_note\": str(str_checker(submission.mod_note)),\\\n",
    "            \"mod_reason_by\": str(str_checker(submission.mod_reason_by)),\\\n",
    "            \"mod_reason_title\": str(str_checker(submission.mod_reason_title)),\\\n",
    "            \"mod_reports\": str(str_checker(submission.mod_reports)),\\\n",
    "            \"num_comments\": int(int_checker(submission.num_comments)),\\\n",
    "            \"num_crossposts\": int(int_checker(submission.num_crossposts)),\\\n",
    "            \"num_duplicates\": int(int_checker(submission.num_duplicates)),\\\n",
    "            \"num_reports\": int(int_checker(submission.num_reports)),\\\n",
    "            \"over_18\": bool(submission.over_18),\\\n",
    "            \"removal_reason\": str(str_checker(submission.removal_reason)),\\\n",
    "            \"removed_by\": str(str_checker(submission.removed_by)),\\\n",
    "            \"removed_by_category\": str(str_checker(submission.removed_by_category)),\\\n",
    "            \"score\": int(int_checker(submission.score)),\\\n",
    "            \"selftext\": replace_chars(str(str_checker(submission.selftext))),\\\n",
    "            \"spoiler\": bool(submission.spoiler),\\\n",
    "            \"title\": replace_chars(str(str_checker(submission.title))),\\\n",
    "            \"top_awarded_type\" : str(str_checker(submission.top_awarded_type)),\\\n",
    "            \"total_awards_received\": int(int_checker(submission.total_awards_received)),\\\n",
    "            \"ups\": int(int_checker(submission.ups)),\\\n",
    "            \"upvote_ratio\": float(int_checker(submission.upvote_ratio)),\\\n",
    "            \"url\": str(str_checker(submission.url)),\\\n",
    "            \"user_reports\": list(list_checker(submission.user_reports)),\n",
    "        }\n",
    "        li.append(temp_dict)\n",
    "    return li\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def stem_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = []\n",
    "    for token, tag in tagged_tokens:\n",
    "        # Convert the POS tag to the WordNet format\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag is None:\n",
    "            # If the POS tag is not recognized, assume it's a noun\n",
    "            wn_tag = wordnet.NOUN\n",
    "        # Apply lemmatization to the token\n",
    "        lemmatized_token = lemmatizer.lemmatize(token, wn_tag)\n",
    "        lemmatized_tokens.append(lemmatized_token)\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "    \n",
    "def stem_and_lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    stem_lem_tokens = []\n",
    "    for token, tag in tagged_tokens:\n",
    "        # Convert the POS tag to the WordNet format\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag is None:\n",
    "            # If the POS tag is not recognized, assume it's a noun\n",
    "            wn_tag = wordnet.NOUN\n",
    "        # Apply stemming and lemmatization to the token\n",
    "        stem_lem_token = lemmatizer.lemmatize(stemmer.stem(token), wn_tag)\n",
    "        stem_lem_tokens.append(stem_lem_token)\n",
    "    return ' '.join(stem_lem_tokens)\n",
    "\n",
    "# Define a helper function to convert POS tags to WordNet format\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag[0] == 'N':\n",
    "        return wordnet.NOUN\n",
    "    elif tag[0] == 'V':\n",
    "        return wordnet.VERB\n",
    "    elif tag[0] == 'J':\n",
    "        return wordnet.ADJ\n",
    "    elif tag[0] == 'R':\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def create_freq_dist(series):\n",
    "    # Tokenize the text in each row of the series\n",
    "    tokens = series.apply(word_tokenize)\n",
    "\n",
    "    # Flatten the list of tokens into a single list\n",
    "    all_tokens = [token for row in tokens for token in row]\n",
    "\n",
    "    # Create a frequency distribution of the tokens\n",
    "    freq_dist = FreqDist(all_tokens)\n",
    "\n",
    "    return freq_dist\n",
    "\n",
    "def frequencyDistribution(txt, word_len = 0):\n",
    "    frequency_dist = nltk.FreqDist(txt)\n",
    "    sorted(frequency_dist, key=frequency_dist.__getitem__, reverse= True)\n",
    "    return dict([k,v] for k, v in frequency_dist.items() if len(k) > word_len)\n",
    "\n",
    "def create_word_cloud(data_frame, file_name):\n",
    "    wordcloud = WordCloud(width=1600, height=800, max_font_size=200, background_color=\"white\").generate_from_frequencies(data_frame)\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(file_name)\n",
    "\n",
    "    plt.savefig(f\"Graphs/B1/{file_name}_wordcloud.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def replace_words(series, replace_dict):\n",
    "    pattern = r'\\b(' + '|'.join(replace_dict.keys()) + r')\\b'\n",
    "    return series.str.replace(pattern, lambda x: replace_dict[x.group()], regex=True)\n",
    "   \n",
    "def run_all(data_frame, file_name):\n",
    "    # Words to lowercase\n",
    "    df_lower_title = data_frame.str.lower()\n",
    "\n",
    "    # Remove stop words\n",
    "    df_lower_title_no_stopwords = df_lower_title.apply(remove_stopwords)\n",
    "\n",
    "    # Apply regex\n",
    "    pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    df_lower_title_no_stopwords = df_lower_title_no_stopwords.str.replace(pattern, '').str.replace(\"'s\", '')\n",
    "    \n",
    "    # Replace characters\n",
    "    replace_dict = {\n",
    "        \"btc\": \"bitcoin\", \"eth\": \"ethereum\", \"u\": \"you\", \"s\":\"\", \"m\": \"am\"\n",
    "    }\n",
    "\n",
    "    df_lower_title_no_stopwords = replace_words(df_lower_title_no_stopwords, replace_dict)\n",
    "    \n",
    "    # Apply steaming\n",
    "    df_lower_title_no_stopwords_steamed = df_lower_title_no_stopwords.apply(stem_text)\n",
    "\n",
    "    # Apply lemmatization\n",
    "    df_lower_title_no_stopwords_lamma = df_lower_title_no_stopwords.apply(lemmatize_text)\n",
    "\n",
    "    # Apply steam and lemmatization\n",
    "    df_lower_title_no_stopwords_steamd_lemma = df_lower_title_no_stopwords.apply(stem_and_lemmatize_text)\n",
    "\n",
    "    # Create frequency distribution\n",
    "    df_lower_title_no_stopwords_steamd_lemma_frequency_dist = create_freq_dist(df_lower_title_no_stopwords_steamd_lemma)\n",
    "    # for item in df_lower_title_no_stopwords_steamd_lemma_frequency_dist.most_common():\n",
    "    #     print(item)\n",
    "    \n",
    "    # Word cloud\n",
    "    create_word_cloud(df_lower_title_no_stopwords_steamd_lemma_frequency_dist, file_name)\n",
    "    \n",
    "def create_big_dataframe(custom_filter_type=\"top_all\",type=\"top\"):\n",
    "    top_ethereum = pd.DataFrame(get_reddit_data(filter_type=custom_filter_type, subreddit_name=\"ethereum\"))\n",
    "    top_bitcoin = pd.DataFrame(get_reddit_data(filter_type=custom_filter_type, subreddit_name=\"Bitcoin\"))\n",
    "    top_cryptocurrency = pd.DataFrame(get_reddit_data(filter_type=custom_filter_type, subreddit_name=\"CryptoCurrency\"))\n",
    "    if((len(top_ethereum) + len(top_bitcoin) + len(top_cryptocurrency)) == len(result)):\n",
    "        print(\"Same lenght\")\n",
    "        frames = [top_ethereum, top_bitcoin, top_cryptocurrency]\n",
    "        result = pd.concat(frames)\n",
    "        result.to_csv(f\"Data/Reddit/{type}_merged_subredits.csv\", index=False)\n",
    "    else:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate polarity on ML/deep learning\n",
    "result = pd.read_csv(\"Data/Reddit/merged_subredits.csv\")\n",
    "run_all(data_frame=result[\"title\"], file_name=\"merged_subredits\")\n",
    "\n",
    "new_result = pd.read_csv(\"Data/Reddit/new_merged_subredits.csv\")\n",
    "run_all(data_frame=new_result[\"title\"], file_name=\"new_merged_subredits\")\n",
    "\n",
    "new_result = pd.read_csv(\"Data/Reddit/top_eth.csv\")\n",
    "run_all(data_frame=new_result[\"title\"], file_name=\"top_eth\")\n",
    "\n",
    "new_result = pd.read_csv(\"Data/Reddit/new_eth.csv\")\n",
    "run_all(data_frame=new_result[\"title\"], file_name=\"new_eth\")\n",
    "\n",
    "# new_result = pd.read_csv(\"Data/Reddit/new_merged_subredits.csv\")\n",
    "# run_all(data_frame=new_result[\"title\"], file_name=\"new_merged_subredits\")\n",
    "\n",
    "# new_result = pd.read_csv(\"Data/Reddit/new_merged_subredits.csv\")\n",
    "# run_all(data_frame=new_result[\"title\"], file_name=\"new_merged_subredits\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-6ac50ba4cc85>:182: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_lower_title_no_stopwords = df_lower_title_no_stopwords.str.replace(pattern, '').str.replace(\"'s\", '')\n"
     ]
    }
   ],
   "source": [
    "read_subreddit = pd.DataFrame(get_reddit_data(filter_type=\"top_all\", subreddit_name=\"ethereum\"))\n",
    "run_all(data_frame=read_subreddit[\"title\"], file_name=\"top_eth\")\n",
    "\n",
    "read_subreddit = pd.DataFrame(get_reddit_data(filter_type=\"new\", subreddit_name=\"ethereum\"))\n",
    "run_all(data_frame=read_subreddit[\"title\"], file_name=\"new_eth\")\n",
    "\n",
    "read_subreddit = pd.DataFrame(get_reddit_data(filter_type=\"top_all\", subreddit_name=\"Bitcoin\"))\n",
    "run_all(data_frame=read_subreddit[\"title\"], file_name=\"top_btc\")\n",
    "\n",
    "read_subreddit = pd.DataFrame(get_reddit_data(filter_type=\"new\", subreddit_name=\"Bitcoin\"))\n",
    "run_all(data_frame=read_subreddit[\"title\"], file_name=\"new_btc\")\n",
    "\n",
    "read_subreddit = pd.DataFrame(get_reddit_data(filter_type=\"top_all\", subreddit_name=\"CryptoCurrency\"))\n",
    "run_all(data_frame=read_subreddit[\"title\"], file_name=\"top_cry\")\n",
    "\n",
    "read_subreddit = pd.DataFrame(get_reddit_data(filter_type=\"new\", subreddit_name=\"CryptoCurrency\"))\n",
    "run_all(data_frame=read_subreddit[\"title\"], file_name=\"new_cry\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create local data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_big_dataframe(custom_filter_type=\"top_all\",type=\"top\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_big_dataframe(custom_filter_type=\"new\",type=\"new\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access News APIs for articles related to the chosen event/campaign (Minimum of 5 articles)\n",
    "\n",
    "• Perform all required cleaning and pre-processing on the articles. </br>\n",
    "• Perform basic descriptive analysis of the collected articles (time distribution, word counts. etc).</br>\n",
    "• Use topic modelling techniques to discover key topics. Display your findings using proper graphs, such as word cloud.</br>\n",
    "• Provide a summary on one of the news articles. Comment on the summarisation quality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from creds import news_api_key\n",
    "from dateutil import parser\n",
    "#from textblob import TextBlob\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd \n",
    "from gensim.summarization import summarize # gensim must be 3.8\n",
    "\n",
    "import requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(api_key, search_items):\n",
    "    # specify the endpoint and parameters for the API request\n",
    "    url = 'https://newsapi.org/v2/everything'\n",
    "    params = {\n",
    "        'apiKey': api_key,\n",
    "        'q': search_items,\n",
    "        'sortBy': 'publishedAt',\n",
    "        'language': 'en'\n",
    "    }\n",
    "    \n",
    "    # send the API request and get the response\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    # extract the title and news source for each article\n",
    "    articles = []\n",
    "    for article in data['articles']:\n",
    "        print(article)\n",
    "        title = article['title'].encode('utf-8').decode('ascii', 'ignore')\n",
    "        source = article['source']['name'].encode('utf-8').decode('ascii', 'ignore')\n",
    "        description = article['description'].encode('utf-8').decode('ascii', 'ignore').replace('\\n', '') if article['description'] else \"\"\n",
    "        url = article['url']\n",
    "        publishedAt = int(datetime.fromisoformat(article['publishedAt'][:-1]).timestamp())\n",
    "        content = article['content'].encode('utf-8').decode('ascii', 'ignore') if article['content'] else \"\"\n",
    "        articles.append({'title': title, 'source': source, 'description': description, 'url': url, 'publishedAt': publishedAt, 'content': content})\n",
    "    \n",
    "    df = pd.DataFrame(articles)\n",
    "    return df\n",
    "\n",
    "def remove_stopwords(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "    df['description'] = df['description'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "    df['content'] = df['content'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "\n",
    "    return df\n",
    "\n",
    "def apply_regex(data_frame):\n",
    "    # define a regular expression pattern to match only alphanumeric characters\n",
    "    pattern = r'[^a-zA-Z0-9 ]'\n",
    "\n",
    "    # apply the regular expression to the title, description, and content columns of the DataFrame\n",
    "    data_frame['title'] = data_frame['title'].apply(lambda x: re.sub(pattern, '', x))\n",
    "    data_frame['description'] = data_frame['description'].apply(lambda x: re.sub(pattern, '', x))\n",
    "    data_frame['content'] = data_frame['content'].apply(lambda x: re.sub(pattern, '', x))\n",
    "    return data_frame\n",
    "\n",
    "def apply_stemming(df):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    df['title'] = df['title'].apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\n",
    "    df['description'] = df['description'].apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\n",
    "    df['content'] = df['content'].apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\n",
    "\n",
    "    return df\n",
    "\n",
    "def apply_lemmatization(df):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    df['title'] = df['title'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n",
    "    df['description'] = df['description'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n",
    "    df['content'] = df['content'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n",
    "\n",
    "    return df\n",
    "\n",
    "def apply_stemming_and_lemmatization(df):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    df['title'] = df['title'].apply(lambda x: ' '.join([lemmatizer.lemmatize(stemmer.stem(word)) for word in word_tokenize(x)]))\n",
    "    df['description'] = df['description'].apply(lambda x: ' '.join([lemmatizer.lemmatize(stemmer.stem(word)) for word in word_tokenize(x)]))\n",
    "    df['content'] = df['content'].apply(lambda x: ' '.join([lemmatizer.lemmatize(stemmer.stem(word)) for word in word_tokenize(x)]))\n",
    "\n",
    "    return df\n",
    "\n",
    "def discover_topics(df, num_topics=5, num_words=10):\n",
    "    data = df[['title', 'description', 'content']].values.tolist()\n",
    "    data = [' '.join(simple_preprocess(str(d))) for d in data]\n",
    "    tokens = [d.split() for d in data]\n",
    "\n",
    "    dictionary = Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "\n",
    "    lda_model = LdaModel(corpus=corpus,\n",
    "                         id2word=dictionary,\n",
    "                         num_topics=num_topics,\n",
    "                         random_state=100,\n",
    "                         chunksize=100,\n",
    "                         passes=10,\n",
    "                         alpha='auto',\n",
    "                         per_word_topics=True)\n",
    "\n",
    "    topics = lda_model.show_topics(num_topics=num_topics, num_words=num_words, formatted=False)\n",
    "\n",
    "    topics_df = pd.DataFrame(columns=['topic_words'])\n",
    "\n",
    "    for topic in topics:\n",
    "        topic_id = topic[0]\n",
    "        topic_words = [word[0] for word in topic[1]]\n",
    "        topic_words = ', '.join(topic_words)\n",
    "        topics_df = topics_df.append({'topic_words': topic_words}, ignore_index=True)\n",
    "\n",
    "    return topics_df\n",
    "        \n",
    "def generate_wordclouds(df):\n",
    "    for col in ['title', 'description', 'content']:\n",
    "        data = df[col].values.tolist()\n",
    "        data = ' '.join([' '.join(str(d).split()) for d in data])\n",
    "        wordcloud = WordCloud(width=800, height=500, background_color='white', max_words=100, contour_width=3, contour_color='steelblue')\n",
    "        wordcloud.generate(data)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(col.capitalize() + ' Word Cloud')\n",
    "        plt.savefig(f\"Graphs/B2/{col}_wordcloud.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "def plot_source_pie(df, n=5):\n",
    "    source_counts = df['source'].value_counts().head(n)\n",
    "    plt.pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%')\n",
    "    plt.title(f\"Top {n} Sources\")\n",
    "    plt.axis('equal')\n",
    "    plt.savefig(f\"Graphs/B2/top_{n}_sources_pie_chart.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_publication_dates(df):\n",
    "    dates = pd.to_datetime(df['publishedAt'], unit='s')\n",
    "    hour_counts = dates.dt.hour.value_counts().sort_index()\n",
    "    hour_counts.plot()\n",
    "    plt.xlabel('Hour of Publication')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.title('Publication Hours')\n",
    "    plt.savefig(\"Graphs/B2/publication_hours.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "def plot_word_count(data_frame):\n",
    "    # Concatenate the 'title', 'description', and 'content' columns into a single Series\n",
    "    text_all = pd.concat([data_frame['title'], data_frame['description'], data_frame['content']]).astype(str).str.lower().str.cat(sep=' ')\n",
    "\n",
    "    # Count the frequency of each word in the text\n",
    "    word_counts_all = Counter(text_all.split())\n",
    "\n",
    "    # Get the top 10 most common words\n",
    "    top_words_all = word_counts_all.most_common(10)\n",
    "\n",
    "    # Loop over each column and create a chart\n",
    "    for col in ['title', 'description', 'content', 'all']:\n",
    "        if col == 'all':\n",
    "            text = text_all\n",
    "            word_counts = word_counts_all\n",
    "        else:\n",
    "            text = data_frame[col].astype(str).str.lower().str.cat(sep=' ')\n",
    "            word_counts = Counter(text.split())\n",
    "\n",
    "        # Get the top 10 most common words for this column\n",
    "        top_words = word_counts.most_common(10)\n",
    "\n",
    "        # Print the word count for this column\n",
    "        word_count = len(text.split())\n",
    "        print(f\"Number of words in {col}: {word_count}\")\n",
    "\n",
    "        # Create a pie chart of the top 10 most common words\n",
    "        plt.pie([count for _, count in top_words], labels=[word for word, _ in top_words])\n",
    "        plt.title(f'Top 10 words in {col}')\n",
    "        plt.savefig(f\"Graphs/B2/top_words_{col}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "def summarize_description(df):\n",
    "    # Check if the DataFrame has a 'description' column\n",
    "    if 'description' not in df.columns:\n",
    "        raise ValueError(\"DataFrame doesn't have a 'description' column.\")\n",
    "\n",
    "    # Iterate over each row in the DataFrame and summarize the text in the 'description' column\n",
    "    summaries = []\n",
    "    for index, row in df.iterrows():\n",
    "        description = row['description']\n",
    "        if len(description.split('.')) == 1:\n",
    "            # If the text contains only one sentence, return it as the summary\n",
    "            summaries.append(description)\n",
    "        else:\n",
    "            # Otherwise, summarize the text using gensim's summarize function\n",
    "            try:\n",
    "                summary = summarize(description, word_count=30)\n",
    "                summaries.append(summary)\n",
    "            except ValueError:\n",
    "                # If the input has no valid sentences, return the original text\n",
    "                summaries.append(description)\n",
    "\n",
    "    # Create a new DataFrame with the summarized text\n",
    "    summary_df = pd.DataFrame({'summary': summaries})\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news = get_news(news_api_key,\"bitcoin\")\n",
    "news_bitcoin = pd.read_csv(\"Data/News/bitcoin.csv\", index_col=0)\n",
    "news_etherium = pd.read_csv(\"Data/News/etherium.csv\", index_col=0)\n",
    "news_crypto = pd.read_csv(\"Data/News/crypto.csv\", index_col=0)\n",
    "news_cryptocurrency = pd.read_csv(\"Data/News/cryptocurrency.csv\", index_col=0)\n",
    "\n",
    "frames = [news_bitcoin, news_etherium, news_crypto, news_cryptocurrency]\n",
    "news_all = pd.concat(frames)\n",
    "\n",
    "# Apply regex\n",
    "news_all = apply_regex(news_all)\n",
    "\n",
    "news_stopwords = remove_stopwords(news_all)\n",
    "\n",
    "apply_stemming(news_stopwords)[[\"title\",\"description\",\"content\"]]\n",
    "apply_lemmatization(news_stopwords)[[\"title\",\"description\",\"content\"]]\n",
    "news_stem_lemma = apply_stemming_and_lemmatization(news_stopwords)[[\"title\",\"description\",\"content\"]]\n",
    "#news_stopwords[[\"title\",\"description\",\"content\"]]\n",
    "topics = discover_topics(news_stopwords, num_topics=6, num_words=10)\n",
    "topics\n",
    "generate_wordclouds(news_stopwords)\n",
    "\n",
    "\n",
    "plot_source_pie(news_all, n=6)\n",
    "plot_publication_dates(news_all)\n",
    "plot_word_count(news_stem_lemma)\n",
    "\n",
    "#summarize_description(news_stem_lemma)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_of_blocks = generate_block_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#big_df = generate_data_frame().rename(columns={\"From\": \"Source\", \"To\": \"Target\", \"Value\": \"Weight\"}).drop(columns=[\"Gas_price\"])\n",
    "#big_df.insert(2, \"Type\", \"directed\")\n",
    "#big_df.to_csv(\"Data/mrg6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = generate_block_list(start_block=1, end_block=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_blocks_in_range(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_blocks_in_range(list_of_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#imported_modules = list(sys.modules.keys())\n",
    "#for module in imported_modules:\n",
    "#    del module\n",
    "#print(imported_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#imported_modules = list(sys.modules.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47de17c9439a8b7f0cca5f84b87c0e7021dcead242cf426ec9dce3b62d8d1f7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
